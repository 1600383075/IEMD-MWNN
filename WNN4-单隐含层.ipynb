{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "import numpy as np \n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计时装饰器\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print('Training time is :{:.2f} s.'.format(end_time - start_time))\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletNeuralNet(object):\n",
    "    # 初始化神经网络，sizes是神经网络的层数和每层神经元个数  \n",
    "    def __init__(self, sizes):\n",
    "        self.sizes_ = sizes\n",
    "        self.num_layers_ = len(sizes)  # 层数\n",
    "        if self.num_layers_ > 3:\n",
    "            print('ERROR!')\n",
    "        self.num_nuerals_ = sizes[1]#隐含层个数 w为\n",
    "        self.w_ = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]  # w_、b_初始化为正态分布随机数\n",
    "        self.b_ = [np.random.randn(y, 1) for y in sizes[1:]]#numpy.random.randn(d0, d1, …, dn)是从标准正态分布中返回一个或多个样本值\n",
    "        self.t_ = np.random.randint(2, 15, (self.num_nuerals_, 1))#np.random.randint(low,high,size))\n",
    "#         self.t_ = np.random.normal(5, 2., (self.num_nuerals_, 1))\n",
    "        self.s_ = 2 * np.random.randn(self.num_nuerals_, 1)\n",
    "    \n",
    "    #欧氏距离\n",
    "    def oushi(self,v1,v2):\n",
    "        v1=np.array(v1);v2=np.array(v2)\n",
    "        op2=np.linalg.norm(v1-v2)\n",
    "        return op2\n",
    "    \n",
    "    # Sigmoid函数，S型曲线，\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    # Sigmoid函数的导函数\n",
    "    def sigmoid_der(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "    \n",
    "    # morlet小波母函数\n",
    "    def phi(self, z, t=1, s=0):\n",
    "        z_ = (z - s) / t\n",
    "        return np.cos(1.75 * z_) * np.exp(-z_**2 / 2.)\n",
    "    \n",
    "    # 小波函数导数\n",
    "    def phi_der(self, z, t=1, s=0):\n",
    "        z_ = (z - s) / t\n",
    "        return (-1.75 * np.sin(1.75 * z_) * np.exp(-z_**2 / 2) - z_ * np.cos(1.75 * z_) * np.exp(-z_**2 / 2)) / t\n",
    "    \n",
    "    def feedforward(self, x): # 前向：各层网络节点计算\n",
    "        n = self.w_[0].shape[1]#输入层个数\n",
    "        x = (np.array(x)).reshape(n,-1)#将数据准换成一行\n",
    "        x1 = self.phi(np.dot(self.w_[0], x) + self.b_[0], self.t_, self.s_)#np.dot点积 小波计算：输入层到输出层\n",
    "        x2 =np.dot(self.w_[1], x1) + self.b_[1]#self.sigmoid(np.dot(self.w_[1], x1) + self.b_[1])#sigmoid计算：\n",
    "        '''隐含层到输出层'''\n",
    "        return x2\n",
    "    \n",
    "    # 反向传播\n",
    "    def backprop(self, x, y):#反向传播更新参数\n",
    "        b_new = [np.ones(b.shape) for b in self.b_]\n",
    "        w_new = [np.ones(w.shape) for w in self.w_]\n",
    "        t_new = self.t_\n",
    "        s_new = self.s_\n",
    "        activation = x\n",
    "        \n",
    "        activations = [x]  # activations代表着每层的输出\n",
    "        #print('activation',np.array(activation))\n",
    "        #print('activation',np.array(activation).size)\n",
    "        zs = []  # zs记录隐含层与输出层的数据\n",
    "        z = np.dot(self.w_[0],activation.reshape(-1,1))+ self.b_[0]#zl(1)\n",
    "        #print('w0',np.array((self.w_[0])).shape)#print('z',np.array(z).shape)\n",
    "        \n",
    "        zs.append(z)#zl(1)\n",
    "        activation = self.phi(z, t_new, s_new)## 计时装饰器\n",
    "\n",
    "        activations.append(activation)#记录了输入数据以及隐含层数据\n",
    "        #print('activation2',np.array(activation).shape)# print('w1',np.array((self.w_[1])).shape)\n",
    "       \n",
    "        z = np.dot(self.w_[1], activation.reshape(-1,1)) + self.b_[1]#zl(2)\n",
    "        print('z',np.array(z).shape)\n",
    "        zs.append(z) \n",
    "        activation = z#self.sigmoid(z)#sigmoid函数计算\n",
    "        '''隐含层到输出层'''\n",
    "        activations.append(activation)#至此记录了输入数据、隐含层数据以及输出数据\n",
    "        \n",
    "        #开始迭代计算\n",
    "        delta = self.cost_derivative(activations[-1], y)#* zs[-1]#self.sigmoid_der(zs[-1])\n",
    "        '''隐含层到输出层'''\n",
    "        b_new[-1] = delta\n",
    "        w_new[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        delta_last = delta.copy()\n",
    "        z = zs[-2]\n",
    "        sp = self.phi_der(z, t_new, s_new)\n",
    "        delta = np.dot(self.w_[-1].transpose(), delta_last) * sp\n",
    "        b_new[-2] =delta\n",
    "\n",
    "        w_new[-2] = np.dot(delta, (activations[-3].reshape(-1,1)).transpose())\n",
    "        sp_t = -.5 * t_new**-1.5 * self.phi((z-s_new) / t_new) - t_new**-2.5 * (z - s_new) * self.phi_der((z - s_new) / t_new)\n",
    "        sp_s = -t_new**-1.5 * self.phi_der((z-s_new) / t_new)\n",
    "        # t_new = np.dot(self.w_[-1].transpose(), delta_last)*sp_t # loss函数对小波函数缩放系数的偏导\n",
    "        # s_new = np.dot(self.w_[-1].transpose(), delta_last)*sp_s # loss函数对小波函数平移系数的偏导\n",
    "        \n",
    "        t_new = delta * sp_t # loss函数对小波函数缩放系数的偏导\n",
    "        s_new = delta * sp_s # loss函数对小波函数平移系数的偏导\n",
    "        \n",
    "        return (b_new, w_new, t_new, s_new)\n",
    "        # 更新权值w，偏移b，缩放因子t，偏移因子s\n",
    "    def update_mini_batch(self,data, mini_batch, lr):\n",
    "        b_new = [np.zeros(b.shape) for b in self.b_]\n",
    "        w_new = [np.zeros(w.shape) for w in self.w_]#a就是输入数据，b就是验证数据\n",
    "        a, b = mini_batch,data\n",
    "        n = np.float(mini_batch.size)\n",
    "        x, y = np.array(a).reshape(-1, 1), np.array(b).reshape(-1, 1)\n",
    "        delta_b_new, delta_w_new, t_new, s_new = self.backprop(x, y)\n",
    "        #print('delta_b_new',(np.array(delta_b_new))[0].shape)\n",
    "        #print('delta_w_new',(np.array(delta_w_new))[0].shape)\n",
    "        b_new = [nb + dnb for nb, dnb in zip(b_new, delta_b_new)]\n",
    "        w_new = [nw + dnw for nw, dnw in zip(w_new, delta_w_new)]\n",
    "           \n",
    "        self.w_ = [w - lr * nw for w, nw in zip(self.w_, w_new)]\n",
    "        self.b_ = [b - lr * nb for b, nb in zip(self.b_, b_new)]\n",
    "        self.t_ = self.t_ - lr * t_new\n",
    "        self.s_ = self.s_ - lr * s_new\n",
    "        #print('权值变化',self.w_[1][0]) #,np.array(self.w_[0]).shape,np.array(self.w_[1]).shape)\n",
    "    \n",
    "    # training_data是训练数据(x, y), epochs是训练次数, mini_batch_size是每次训练样本数, lr是learning rate，step是展示的迭代间隔\n",
    "    @timer\n",
    "    def SGD(self, training_data, epochs=50, mini_batch_size=50, lr=.1, step=10):\n",
    "        assert type(step) == int, 'Step must be a integer.'\n",
    "        n = training_data.shape[0]#数据个数\n",
    "        juli=100\n",
    "        i=0\n",
    "        for j in range(epochs):\n",
    "            ss = training_data\n",
    "            i+=1\n",
    "            print('第%d次循环'%(i))\n",
    "            for k in range(0, n-mini_batch_size, mini_batch_size):\n",
    "                j=1\n",
    "                while juli>0.001 and j<1000:\n",
    "                    np.random.shuffle(ss)#np.random.shuffle(x) 现场修改序列，改变自身内容。（类似洗牌，打乱顺序\n",
    "                    mini_batches =ss[k:k + mini_batch_size]\n",
    "                    #print('mini_batches' ,np.array(mini_batches).shape,mini_batches[0] )\n",
    "                    cs0=ss[k+mini_batch_size:k+mini_batch_size+ 10] #此处对测试进行修改\n",
    "                    #print('cs0' ,cs0 )\n",
    "                    #print('cs0' ,np.array(cs0).shape,cs0[0] )\n",
    "                    #print(np.array(cs0[0]).reshape(-1,1))\n",
    "                    mini_batch=mini_batches\n",
    "                    cs=cs0\n",
    "                    self.update_mini_batch(cs, mini_batch, lr)\n",
    "                    aa=self.feedforward(mini_batch)\n",
    "                    print('输出结果',aa)\n",
    "                    juli=self.oushi(aa,cs)\n",
    "                    print('误差大小%d'%juli)\n",
    "                    j+=1\n",
    "                    print('内循环次数%d'%j)\n",
    "        \n",
    "        #print('第%d次循环',%i)\n",
    "                \n",
    "            '''\n",
    "            print(self.oushi(aa,cs))\n",
    "            d1=self.oushi(aa,cs)\n",
    "            accur = self.evaluate(training_data) * 100\n",
    "            mse_loss = self.mse_loss(training_data)\n",
    "            if (j + 1) % step == 0 or j == 0:\n",
    "                print(\"Epoch {0}, mse_loss: {1:.4f}, accury on the training set :{2:.2f}{3}\".format(j+1, mse_loss, accur, '%'))\n",
    "            # print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(training_data), n))\n",
    "            '''\n",
    "# 计算正确率\n",
    "    def evaluate(self, data):\n",
    "        x_t, x_label = data\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in zip(list(x_t), list(x_label))]\n",
    "        acc = sum(int(x == y) for (x, y) in test_results) / x_t.shape[0]\n",
    "        return acc\n",
    "    \n",
    "    # mse_loss的导数\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations - y)\n",
    "     # mse_loss\n",
    "    def mse_loss(self, training_data):\n",
    "        x_t,x_label = training_data\n",
    "        test_results = [.5 * norm(self.feedforward(x).flatten() - self.one_hot(y, num_classes=10))**2\n",
    "                        for (x, y) in zip(list(x_t), list(x_label))]\n",
    "        return np.array(test_results).mean()\n",
    "    \n",
    "    # 预测\n",
    "    def predict(self, data):\n",
    "        data = data.reshape(-1, self.sizes_[0])\n",
    "        value = np.array([np.argmax(net.feedforward(x)) for x in data], dtype='uint8')\n",
    "        return value\n",
    "    \n",
    "    # 保存训练模型\n",
    "    def save(self):\n",
    "        pass  # 把_w和_b保存到文件(pickle)  \n",
    "    \n",
    "    def load(self):\n",
    "        pass            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a0=pd.read_csv(r'C:\\Users\\stu\\Desktop\\xlab_时许数据\\CPU.csv').values\n",
    "a0[:,1].size;a1=a0[:,1];a1=a1.tolist();type(a1)\n",
    "x=a1[:500]\n",
    "x2=a1[500:510]\n",
    "training_data=np.array(x)\n",
    "print(training_data.size)\n",
    "type(training_data)\n",
    "#sizes=[len(x), 128, 10]\n",
    "#sizes_ = sizes\n",
    "#num_layers_ = len(sizes)\n",
    "#print(num_layers_)# 层数\n",
    "#if num_layers_ > 3:\n",
    "    #print('ERROR!')\n",
    "#num_nuerals_ = sizes[1]#隐含层个数\n",
    "#print(num_nuerals_)\n",
    "#w_ = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]  \n",
    "#b_ = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "#t_ = np.random.randint(2, 15, (num_nuerals_, 1))\n",
    "#s_ = 2 * np.random.randn(num_nuerals_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.70267289]\n",
      " [-0.07643722]\n",
      " [-0.32553032]\n",
      " [-2.20032755]\n",
      " [-0.14839752]\n",
      " [ 0.015463  ]\n",
      " [ 1.14991599]\n",
      " [ 1.60360309]\n",
      " [-0.329563  ]\n",
      " [-0.10401748]]\n",
      "[[58.06666667]\n",
      " [57.68333333]\n",
      " [57.28475717]\n",
      " [58.1       ]\n",
      " [58.03333333]\n",
      " [57.6       ]\n",
      " [57.5       ]\n",
      " [57.91666667]\n",
      " [58.45      ]\n",
      " [58.41528051]]\n",
      "xx2 [[58.06666667]\n",
      " [57.68333333]\n",
      " [57.28475717]\n",
      " [58.1       ]\n",
      " [58.03333333]\n",
      " [57.6       ]\n",
      " [57.5       ]\n",
      " [57.91666667]\n",
      " [58.45      ]\n",
      " [58.41528051]]\n",
      "z (10, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
       "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.76588728e-13,  2.78973205e-13,  2.74263715e-13, ...,\n",
       "          2.77292667e-13,  2.76741419e-13,  2.78255845e-13],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
       "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00]]),\n",
       " array([[-0.00000000e+00,  0.00000000e+00, -0.00000000e+00, ...,\n",
       "          0.00000000e+00,  1.53136932e-15, -0.00000000e+00],\n",
       "        [-0.00000000e+00,  0.00000000e+00, -0.00000000e+00, ...,\n",
       "          0.00000000e+00,  1.50506270e-15, -0.00000000e+00],\n",
       "        [-0.00000000e+00,  0.00000000e+00, -0.00000000e+00, ...,\n",
       "          0.00000000e+00,  1.50116758e-15, -0.00000000e+00],\n",
       "        ...,\n",
       "        [-0.00000000e+00,  0.00000000e+00, -0.00000000e+00, ...,\n",
       "          0.00000000e+00,  1.46736545e-15, -0.00000000e+00],\n",
       "        [-0.00000000e+00,  0.00000000e+00, -0.00000000e+00, ...,\n",
       "          0.00000000e+00,  1.53163572e-15, -0.00000000e+00],\n",
       "        [-0.00000000e+00,  0.00000000e+00, -0.00000000e+00, ...,\n",
       "          0.00000000e+00,  1.52485392e-15, -0.00000000e+00]])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = WaveletNeuralNet([50,128,10])\n",
    "bf=net.feedforward(np.array(x)[0:50])\n",
    "print(bf)\n",
    "#print(bf)\n",
    "x2=a1[500:510]\n",
    "xx=pd.DataFrame(x2)\n",
    "print(xx.values)\n",
    "xx2=xx.values\n",
    "print('xx2',xx2)\n",
    "[n1,n2,n3,n4]=net.backprop(np.array(x[0:50]),xx2)\n",
    "np.array(n2)[1].shape\n",
    "n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = WaveletNeuralNet([50,500,10])\n",
    "net.SGD(training_data, epochs=2, mini_batch_size=50, lr=.1, step=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[58.04749659],\n",
       "       [59.10030716],\n",
       "       [57.86762371],\n",
       "       [57.98580398],\n",
       "       [58.1100422 ],\n",
       "       [57.87074189],\n",
       "       [58.01195216],\n",
       "       [57.85867528],\n",
       "       [57.99325519],\n",
       "       [58.00245571]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.feedforward(np.array(x)[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc1=a1[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc=[57.92731745,57.92206549,57.94790921,57.94092723,58.09708394,57.80595973,57.8869547,57.90202747,57.74416407,58.08103361]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6815586036409542"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.oushi(cc1,cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 result:0.7420881111907824, result:0.7752849682944595\n",
      "1 result:0.7324768705997207, result:0.7775819271030018\n",
      "2 result:0.722538848562922, result:0.7798215827742319\n",
      "3 result:0.7122848772449228, result:0.7820060734063023\n",
      "4 result:0.7017292761322197, result:0.7841374389249309\n",
      "5 result:0.6908899293110883, result:0.7862176255712456\n",
      "6 result:0.6797882700461756, result:0.7882484901472318\n",
      "7 result:0.6684491619864679, result:0.7902318040469472\n",
      "8 result:0.6569006710842666, result:0.7921692571023269\n",
      "9 result:0.6451737282231507, result:0.7940624612716817\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def sigmoidDerivationx(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 初始化一些参数\n",
    "    alpha = 0.5\n",
    "    numIter = 10#迭代次数\n",
    "    w1 = [[0.15, 0.20], [0.25, 0.30]]  # Weight of input layer\n",
    "    w2 = [[0.40, 0.45], [0.50, 0.55]]\n",
    "    # print(np.array(w2).T)\n",
    "    b1 = 0.35\n",
    "    b2 = 0.60\n",
    "    x = [0.05, 0.10]\n",
    "    y = [0.01, 0.99]\n",
    "    # 前向传播\n",
    "    z1 = np.dot(w1, x) + b1     # dot函数是常规的矩阵相乘\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    z2 = np.dot(w2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    for n in range(numIter):\n",
    "        # 反向传播 使用代价函数为C=1 / (2n) * sum[(y-a2)^2]\n",
    "        # 分为两次\n",
    "        # 一次是最后一层对前面一层的错误\n",
    "\n",
    "        delta2 = np.multiply(-(y-a2), np.multiply(a2, 1-a2))\n",
    "        # for i in range(len(w2)):\n",
    "        #     print(w2[i] - alpha * delta2[i] * a1)\n",
    "        #计算非最后一层的错误\n",
    "        # print(delta2)\n",
    "        delta1 = np.multiply(np.dot(np.array(w2).T, delta2), np.multiply(a1, 1-a1))\n",
    "        # print(delta1)\n",
    "        # for i in range(len(w1)):\n",
    "            # print(w1[i] - alpha * delta1[i] * np.array(x))\n",
    "        #更新权重\n",
    "        for i in range(len(w2)):\n",
    "            w2[i] = w2[i] - alpha * delta2[i] * a1\n",
    "        for i in range(len(w1)):\n",
    "            w1[i] = w1[i] - alpha * delta1[i] * np.array(x)\n",
    "        #继续前向传播，算出误差值\n",
    "        z1 = np.dot(w1, x) + b1\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = np.dot(w2, a1) + b2\n",
    "        a2 = sigmoid(z2)\n",
    "        print(str(n) + \" result:\" + str(a2[0]) + \", result:\" +str(a2[1]))\n",
    "        # print(str(n) + \"  error1:\" + str(y[0] - a2[0]) + \", error2:\" +str(y[1] - a2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=1;bb=2\n",
    "i=0;j=1\n",
    "while aa<100 and j<20:\n",
    "    aa+=1\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
